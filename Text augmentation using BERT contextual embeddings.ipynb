{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2066bc59",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a09b857",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5796f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "import nlpaug.augmenter.word.context_word_embs as aug\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0df38a",
   "metadata": {},
   "source": [
    "## Text Cleanup Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e26e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    number_pattern = r\"\\d+\"\n",
    "    without_number = re.sub(pattern=number_pattern, repl=\" \", string=text)\n",
    "    return without_number\n",
    "\n",
    "def lemmatizing(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        lemma_word = lemmatizer.lemmatize(tokens[i])\n",
    "        tokens[i] = lemma_word\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    removed = []\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    tokens = word_tokenize(text)\n",
    "    for i in range(len(tokens)):\n",
    "        if tokens[i] not in stop_words:\n",
    "            removed.append(tokens[i])\n",
    "    return \" \".join(removed)\n",
    "\n",
    "def remove_extra_white_spaces(text):\n",
    "    single_char_pattern = r\"\\s+[a-zA-Z]\\s+\"\n",
    "    without_sc = re.sub(pattern=single_char_pattern, repl=\" \", string=text)\n",
    "    return without_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7912eb",
   "metadata": {},
   "source": [
    "## Data Import and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fname = os.path.join(\"<<your path and input excel file name with extension>>\")\n",
    "df = pd.read_excel (fname)\n",
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Comments', 'label']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5de742",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d925742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Comments'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df.label == ''].index)\n",
    "df = df.drop(df[df.Comments == ''].index)\n",
    "\n",
    "df = df.drop(df[df.label.isnull()].index)\n",
    "df = df.drop(df[df.Comments.isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01720a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Comments'] = df['Comments'].apply(lambda x: convert_to_lower(x))\n",
    "df['Comments'] = df['Comments'].apply(lambda x: remove_numbers(x))\n",
    "df['Comments'] = df['Comments'].apply(lambda x: remove_punctuation(x))\n",
    "df['Comments'] = df['Comments'].apply(lambda x: remove_stopwords(x))\n",
    "df['Comments'] = df['Comments'].apply(lambda x: remove_extra_white_spaces(x))\n",
    "df['Comments'] = df['Comments'].apply(lambda x: lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df.label == ''].index)\n",
    "df = df.drop(df[df.Comments == ''].index)\n",
    "\n",
    "df = df.drop(df[df.label.isnull()].index)\n",
    "df = df.drop(df[df.Comments.isnull()].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2168fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.8, random_state=1)\n",
    "test_df = df.drop(train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34795766",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.value_counts('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0d4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_excel(\"<<your path and output excel file name with extension for the hold-out data>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5bee82",
   "metadata": {},
   "source": [
    "## Use BERT contextual embeddings augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7105fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = aug.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560db651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentData(df, augmenter, repetitions, num_samples, label):\n",
    "    augmented_texts = []\n",
    "    imbalanced_class_df = df[df['label'] == label].reset_index(drop=True)\n",
    "    for i in tqdm(np.random.randint(0, len(imbalanced_class_df), num_samples)):\n",
    "        # generating 'num_samples' augmented texts\n",
    "        for _ in range(repetitions):\n",
    "            augmented_text = augmenter.augment(imbalanced_class_df['Comments'].iloc[i])\n",
    "            augmented_texts.append(augmented_text)\n",
    "    \n",
    "    data = {\n",
    "        'Comments': augmented_texts,\n",
    "        'label': label\n",
    "    }\n",
    "    aug_df = pd.DataFrame(data)\n",
    "    df = shuffle(df.append(aug_df).reset_index(drop=True))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88904a",
   "metadata": {},
   "source": [
    "## Check if text augmentation works for a few samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02347616",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = train_df['Comments'].iloc[100]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44442293",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter = aug.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
    "augmented_sample_text = augmenter.augment(sample_text)\n",
    "augmented_sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11790048",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(augmenter.augment(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f407743",
   "metadata": {},
   "source": [
    "## Augment the data for each of the minority classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b06963",
   "metadata": {},
   "source": [
    "To balance the data, the output classes need not have exactly the same number of records. They just need to have approximately the same number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8471f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = augmentData(train_df, augmenter, 1, 1500, \"Output minority class 1\")\n",
    "aug_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3439be",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = augmentData(aug_df, augmenter, 1, 1500, \"Output minority class 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = augmentData(aug_df, augmenter, 1, 1500, \"Output minority class 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d99453",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = augmentData(aug_df, augmenter, 1, 1500, \"Output minority class 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f64789",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = augmentData(aug_df, augmenter, 1, 1500, \"Output minority class 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df.to_excel(\"<<your path and output excel file name with extension for the augmented training data>>\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730514b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
